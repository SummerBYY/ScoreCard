当我们通过提出假设模型，参数估计等过程得出一个经验回归方程
```math
y = \beta_0+\beta_1x_1+\beta_2x_2+···+\beta_kx_k+\epsilon
```
还不能直接用它去做分析和预测，还需对回归方程进行检验，也即模型检验。参数估计是通过样本量估计总体参数，而假设检验是先对总体参数提出一个假设，然后利用样本信息去检验这个假设是否成立。
模型检验分位两部分，一是系数检验，常用方式是T检验；二是方程检验，常用方法是方差分析，也叫F检验。

对回归方程进行检验时，通常是基于三大假设中的正态性假设即
```math
\epsilon_i ~ N(0,\delta^2)
```
#### 回归显著性检验
回归显著性检验是对模型适应性的整体检验，检验的是响应变量y与所有回归变量`$x_1,x_2.....,x_k$`之间是否存在线性关系。假设原则是

```math
H_0:\beta_0=\beta_1=···=\beta_k=0  

对立假设：H_1:\beta_j \neq 0  (至少存在一个j)

```
如果原假设`$H_0$`不成立，则意味着回归变量中，至少有一个对模型有显著性贡献。
> 显著性，依据统计学中的小概率原理，即小概率事件是不可能事件。若原假设被证明是小概率事件，则否定原假设。

由之前一元线性回归方差分析的推论，我们知道


```math
SS_总 = SS_回+SS_残
```
即总平方和=回归平方和+残差平方和。如果原假设为真，则`$SS_回/\delta^2服从\chi_k^2$`分布，其自由度为模型回归变量个数k。`$SS_残/\delta^2 ~\chi_{n-k-1}^2$`,且`$SS_残与SS_回$`独立。所以F统计量记为
```math
F={{SS_回/k}\over{SS_残/(n-k-1)}}={MS_回\over{MS_残}}
```
服从`$F_{k,n-k-1}$`分布。


```math
E(MS_残)=\delta^2

E(MS_回)=\delta^2+{{{\beta^{*\prime}X_c^\prime X_c\beta^*}}\over{k\delta^2}}
```

